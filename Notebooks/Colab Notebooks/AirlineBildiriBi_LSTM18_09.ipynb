{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Embedding, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "QQV3cEGOFxDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htSyVR_SFOy_",
        "outputId": "f2ae2723-6f71-4a8f-e060-b27d555890af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.29.184.26:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Convert Data for TPU\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 1: Load the Preprocessed Data from Google Drive\n",
        "df = pd.read_csv('/content/drive/MyDrive/yuksekTez/airline_dataset/preprocessed_data18-09.csv')\n",
        "X = df.drop(columns=['label']).values\n",
        "y = df['label'].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHZTF0wkFywM",
        "outputId": "459b1147-5b3e-4643-f92d-76ae476c75a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Split Data into Training, Validation, and Test Sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "Dcavg25_F0-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "    X_train_tpu = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "    y_train_tpu = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
        "    X_val_tpu = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
        "    y_val_tpu = tf.convert_to_tensor(y_val, dtype=tf.int32)\n",
        "    X_test_tpu = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "    y_test_tpu = tf.convert_to_tensor(y_test, dtype=tf.int32)"
      ],
      "metadata": {
        "id": "YYI-kMR_F4u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'input_dim' is the dimension of your input features\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "# Step 4: Define and Compile the Bi-LSTM Model\n",
        "with strategy.scope():\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=input_dim, output_dim=64, input_length=input_dim),\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        Dropout(0.5),\n",
        "        Bidirectional(LSTM(32)),\n",
        "        Dense(3, activation='softmax')  # Assuming 3 classes (positive, neutral, negative)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy', 'sparse_categorical_crossentropy'])"
      ],
      "metadata": {
        "id": "iV3sNyMqF6g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp32Ruv2F8uc",
        "outputId": "3eedb17d-75ff-446d-ef64-e5f2f5046d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1536, 64)          98304     \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 1536, 128)        66048     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1536, 128)         0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 64)               41216     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 205,763\n",
            "Trainable params: 205,763\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Train the Bi-LSTM Model\n",
        "with strategy.scope():\n",
        "    history = model.fit(\n",
        "        X_train_tpu,\n",
        "        y_train_tpu,\n",
        "        epochs=15,  # Increase the number of epochs\n",
        "        validation_data=(X_val_tpu, y_val_tpu),\n",
        "        batch_size=64\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WMA9dN-F-hs",
        "outputId": "9989a770-21df-4377-8544-dd1e35c09c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "323/323 [==============================] - 291s 863ms/step - loss: nan - accuracy: 0.3360 - sparse_categorical_crossentropy: nan - val_loss: nan - val_accuracy: 0.3246 - val_sparse_categorical_crossentropy: nan\n",
            "Epoch 2/15\n",
            "323/323 [==============================] - 269s 833ms/step - loss: nan - accuracy: 0.3363 - sparse_categorical_crossentropy: nan - val_loss: nan - val_accuracy: 0.3246 - val_sparse_categorical_crossentropy: nan\n",
            "Epoch 3/15\n",
            "323/323 [==============================] - 269s 833ms/step - loss: nan - accuracy: 0.3363 - sparse_categorical_crossentropy: nan - val_loss: nan - val_accuracy: 0.3246 - val_sparse_categorical_crossentropy: nan\n",
            "Epoch 4/15\n",
            "323/323 [==============================] - 269s 832ms/step - loss: nan - accuracy: 0.3363 - sparse_categorical_crossentropy: nan - val_loss: nan - val_accuracy: 0.3246 - val_sparse_categorical_crossentropy: nan\n",
            "Epoch 5/15\n",
            "323/323 [==============================] - 269s 832ms/step - loss: nan - accuracy: 0.3363 - sparse_categorical_crossentropy: nan - val_loss: nan - val_accuracy: 0.3246 - val_sparse_categorical_crossentropy: nan\n",
            "Epoch 6/15\n",
            "323/323 [==============================] - 269s 833ms/step - loss: nan - accuracy: 0.3363 - sparse_categorical_crossentropy: nan - val_loss: nan - val_accuracy: 0.3246 - val_sparse_categorical_crossentropy: nan\n",
            "Epoch 7/15\n",
            "323/323 [==============================] - 269s 831ms/step - loss: nan - accuracy: 0.3363 - sparse_categorical_crossentropy: nan - val_loss: nan - val_accuracy: 0.3246 - val_sparse_categorical_crossentropy: nan\n",
            "Epoch 8/15\n",
            "323/323 [==============================] - 269s 831ms/step - loss: nan - accuracy: 0.3363 - sparse_categorical_crossentropy: nan - val_loss: nan - val_accuracy: 0.3246 - val_sparse_categorical_crossentropy: nan\n",
            "Epoch 9/15\n",
            "323/323 [==============================] - 269s 832ms/step - loss: nan - accuracy: 0.3363 - sparse_categorical_crossentropy: nan - val_loss: nan - val_accuracy: 0.3246 - val_sparse_categorical_crossentropy: nan\n",
            "Epoch 10/15\n",
            "323/323 [==============================] - 269s 832ms/step - loss: nan - accuracy: 0.3363 - sparse_categorical_crossentropy: nan - val_loss: nan - val_accuracy: 0.3246 - val_sparse_categorical_crossentropy: nan\n",
            "Epoch 11/15\n",
            " 44/323 [===>..........................] - ETA: 3:45 - loss: nan - accuracy: 0.3299 - sparse_categorical_crossentropy: nan"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Step 6: Evaluate the Model\n",
        "with strategy.scope():\n",
        "    loss, accuracy = model.evaluate(X_test_tpu, y_test_tpu)\n",
        "    y_pred = model.predict(X_test_tpu)\n",
        "    y_pred = tf.argmax(y_pred, axis=1)\n",
        "    f1 = f1_score(y_test_tpu.numpy(), y_pred.numpy(), average='weighted')\n",
        "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
        "    print(f'Weighted F1 Score: {f1:.4f}')\n",
        "\n",
        "    # Compute and print classification report\n",
        "    report = classification_report(y_test_tpu.numpy(), y_pred.numpy(), target_names=['Positive', 'Neutral', 'Negative'])\n",
        "    print(f'Classification Report:\\n{report}')\n",
        "\n",
        "    # Compute and print confusion matrix\n",
        "    cm = confusion_matrix(y_test_tpu.numpy(), y_pred.numpy())\n",
        "    print(f'Confusion Matrix:\\n{cm}')"
      ],
      "metadata": {
        "id": "m0vMWl5dGARM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Visualize the Results\n",
        "# Plot training and validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F011L_cpGBpc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}