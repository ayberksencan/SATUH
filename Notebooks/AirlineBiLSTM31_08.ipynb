{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhBoL1jHigkr",
        "outputId": "70112c21-a7d8-459c-b6d9-08255d973836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.119.245.146:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "strategy = tf.distribute.TPUStrategy(tpu)\n",
        "\n",
        "# Step 1: Import Libraries and Load the Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset from a CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/yuksekTez/airline_dataset/clean_text.csv')\n",
        "\n",
        "with strategy.scope():\n",
        "    # Step 2: Preprocess the Dataset\n",
        "    tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(df[\"text\"])\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    sequences = tokenizer.texts_to_sequences(df[\"text\"])\n",
        "    maxlen = 100\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding=\"post\", truncating=\"post\")\n",
        "    labels = df[\"airline_sentiment\"]\n",
        "\n",
        "    label_binarizer = LabelBinarizer()\n",
        "    labels = label_binarizer.fit_transform(labels)\n",
        "    num_classes = len(label_binarizer.classes_)\n",
        "\n",
        "# Step 3: Split the Dataset into Training and Testing Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.25, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "\n",
        "# Learning rate schedule function\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch < 20:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * 0.1\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Learning rate scheduler callback\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "with strategy.scope():\n",
        "    # Define the model\n",
        "    model2 = Sequential()\n",
        "    model2.add(Embedding(vocab_size, 64, input_length=maxlen))  # Increased embedding size\n",
        "    model2.add(Bidirectional(LSTM(128, return_sequences=True)))  # Bi-LSTM layer\n",
        "    model2.add(Dropout(0.4))  # Increased dropout rate\n",
        "    model2.add(Bidirectional(LSTM(64)))  # Additional Bi-LSTM layer\n",
        "    model2.add(Dense(128, activation='relu'))\n",
        "    model2.add(Dropout(0.5))  # Increased dropout rate\n",
        "    model2.add(BatchNormalization())\n",
        "    model2.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model2.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "    # Print model summary\n",
        "    model2.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model2.fit(X_train, y_train,\n",
        "                     epochs=100,\n",
        "                     batch_size=64,\n",
        "                     validation_split=0.1,\n",
        "                     callbacks=[early_stopping, lr_scheduler])\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model2.predict(X_test)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "y_true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(classification_report(y_true_labels, y_pred_labels))\n",
        "print(confusion_matrix(y_true_labels, y_pred_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rnz4xD8bikWv",
        "outputId": "97418672-9ed0-4dc8-a385-f700763bd33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 100, 64)           1009280   \n",
            "                                                                 \n",
            " bidirectional_8 (Bidirectio  (None, 100, 256)         197632    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 100, 256)          0         \n",
            "                                                                 \n",
            " bidirectional_9 (Bidirectio  (None, 128)              164352    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 128)              512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,388,675\n",
            "Trainable params: 1,388,419\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "155/155 [==============================] - 29s 102ms/step - loss: 0.7477 - accuracy: 0.6793 - val_loss: 0.8204 - val_accuracy: 0.6166 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "155/155 [==============================] - 7s 45ms/step - loss: 0.4961 - accuracy: 0.8163 - val_loss: 0.6296 - val_accuracy: 0.7259 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "155/155 [==============================] - 7s 46ms/step - loss: 0.3907 - accuracy: 0.8615 - val_loss: 0.5686 - val_accuracy: 0.7668 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "155/155 [==============================] - 7s 45ms/step - loss: 0.3047 - accuracy: 0.8978 - val_loss: 0.6776 - val_accuracy: 0.7750 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "155/155 [==============================] - 7s 45ms/step - loss: 0.2655 - accuracy: 0.9141 - val_loss: 0.6952 - val_accuracy: 0.7878 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "155/155 [==============================] - 7s 46ms/step - loss: 0.2350 - accuracy: 0.9279 - val_loss: 0.8427 - val_accuracy: 0.7632 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "155/155 [==============================] - 7s 46ms/step - loss: 0.2017 - accuracy: 0.9420 - val_loss: 0.8221 - val_accuracy: 0.7723 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "155/155 [==============================] - 7s 45ms/step - loss: 0.1737 - accuracy: 0.9475 - val_loss: 0.9267 - val_accuracy: 0.7550 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "155/155 [==============================] - 7s 46ms/step - loss: 0.1691 - accuracy: 0.9524 - val_loss: 0.9896 - val_accuracy: 0.7714 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "155/155 [==============================] - 7s 46ms/step - loss: 0.1541 - accuracy: 0.9574 - val_loss: 1.0128 - val_accuracy: 0.7723 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "155/155 [==============================] - 7s 46ms/step - loss: 0.1324 - accuracy: 0.9622 - val_loss: 1.1324 - val_accuracy: 0.7678 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "155/155 [==============================] - 7s 46ms/step - loss: 0.1342 - accuracy: 0.9626 - val_loss: 1.1474 - val_accuracy: 0.7778 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "155/155 [==============================] - 8s 49ms/step - loss: 0.1157 - accuracy: 0.9706 - val_loss: 1.2505 - val_accuracy: 0.7769 - lr: 0.0010\n",
            "115/115 [==============================] - 6s 33ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.98      0.86      2340\n",
            "           1       0.72      0.34      0.46       738\n",
            "           2       0.89      0.48      0.63       582\n",
            "\n",
            "    accuracy                           0.77      3660\n",
            "   macro avg       0.79      0.60      0.65      3660\n",
            "weighted avg       0.77      0.77      0.74      3660\n",
            "\n",
            "[[2285   45   10]\n",
            " [ 465  250   23]\n",
            " [ 249   53  280]]\n"
          ]
        }
      ]
    }
  ]
}